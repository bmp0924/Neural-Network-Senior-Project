{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             {metadata,data,mp3_metadata,clips,normalize,zips}\n",
      "                             ...\n",
      "ipykernel_launcher.py: error: invalid choice: 'C:\\\\Users\\\\Roger\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-95b2ed83-fde4-494a-bc2a-b890d6a9ac14.json' (choose from 'metadata', 'data', 'mp3_metadata', 'clips', 'normalize', 'zips')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# FMA: A Dataset For Music Analysis\n",
    "# Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import zipfile\n",
    "import subprocess as sp\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import mutagen\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "TIME = datetime(2017, 4, 1).timestamp()\n",
    "\n",
    "README = \"\"\"This .zip archive is part of the FMA, a dataset for music analysis.\n",
    "Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "Code & data: https://github.com/mdeff/fma\n",
    "Paper: https://arxiv.org/abs/1612.01840\n",
    "Each .mp3 is licensed by its artist.\n",
    "The content's integrity can be verified with sha1sum -c checksums.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def download_metadata(args):\n",
    "\n",
    "    fma = utils.FreeMusicArchive(os.environ.get('FMA_KEY'))\n",
    "\n",
    "    if args.tid_max is None:\n",
    "        args.tid_max = int(fma.get_recent_tracks()[0][0])\n",
    "\n",
    "    message = 'Collecting metadata from track ID {} to {}.'\n",
    "    print(message.format(args.tid_min, args.tid_max))\n",
    "\n",
    "    not_found = {}\n",
    "\n",
    "    id_range = trange(args.tid_min, args.tid_max, desc='tracks')\n",
    "    tracks, not_found['tracks'] = fma.get_all('track', id_range)\n",
    "\n",
    "    id_range = tqdm(tracks['album_id'].unique(), desc='albums')\n",
    "    albums, not_found['albums'] = fma.get_all('album', id_range)\n",
    "\n",
    "    id_range = tqdm(tracks['artist_id'].unique(), desc='artists')\n",
    "    artists, not_found['artists'] = fma.get_all('artist', id_range)\n",
    "\n",
    "    genres = fma.get_all_genres()\n",
    "\n",
    "    for dataset in 'tracks', 'albums', 'artists', 'genres':\n",
    "        eval(dataset).sort_index(axis=0, inplace=True)\n",
    "        eval(dataset).sort_index(axis=1, inplace=True)\n",
    "        eval(dataset).to_csv('raw_' + dataset + '.csv')\n",
    "        if dataset != 'genres':\n",
    "            print('{}: {} collected, {} not found'.format(\n",
    "                dataset, len(eval(dataset)), len(not_found[dataset])))\n",
    "\n",
    "    pickle.dump(not_found, open('not_found.pickle', 'wb'))\n",
    "\n",
    "\n",
    "def _create_subdirs(dst_dir, tracks):\n",
    "\n",
    "    # Get write access.\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    os.chmod(dst_dir, 0o777)\n",
    "\n",
    "    # Create writable sub-directories.\n",
    "    n_folders = max(tracks.index) // 1000 + 1\n",
    "    for folder in range(n_folders):\n",
    "        dst = os.path.join(dst_dir, '{:03d}'.format(folder))\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "        os.chmod(dst, 0o777)\n",
    "\n",
    "\n",
    "def download_data(args):\n",
    "\n",
    "    dst_dir = os.path.join(os.path.abspath(args.path), 'fma_full')\n",
    "    tracks = pd.read_csv('raw_tracks.csv', index_col=0)\n",
    "    _create_subdirs(dst_dir, tracks)\n",
    "\n",
    "    fma = utils.FreeMusicArchive(os.environ.get('FMA_KEY'))\n",
    "    not_found = pickle.load(open('not_found.pickle', 'rb'))\n",
    "    not_found['audio'] = []\n",
    "\n",
    "    # Download missing tracks.\n",
    "    collected = 0\n",
    "    for tid in tqdm(tracks.index):\n",
    "        dst = utils.get_audio_path(dst_dir, tid)\n",
    "        if not os.path.exists(dst):\n",
    "            try:\n",
    "                fma.download_track(tracks.at[tid, 'track_file'], dst)\n",
    "                collected += 1\n",
    "            except:  # requests.HTTPError\n",
    "                not_found['audio'].append(tid)\n",
    "\n",
    "    pickle.dump(not_found, open('not_found.pickle', 'wb'))\n",
    "\n",
    "    existing = len(tracks) - collected - len(not_found['audio'])\n",
    "    print('audio: {} collected, {} existing, {} not found'.format(\n",
    "        collected, existing, len(not_found['audio'])))\n",
    "\n",
    "\n",
    "def _extract_metadata(tid, path):\n",
    "    \"\"\"Extract metadata from one audio file.\"\"\"\n",
    "\n",
    "    metadata = pd.Series(name=tid)\n",
    "\n",
    "    try:\n",
    "        path = utils.get_audio_path(path, tid)\n",
    "        f = mutagen.File(path)\n",
    "        x, sr = librosa.load(path, sr=None, mono=False)\n",
    "        assert f.info.channels == (x.shape[0] if x.ndim > 1 else 1)\n",
    "        assert f.info.sample_rate == sr\n",
    "\n",
    "        mode = {\n",
    "            mutagen.mp3.BitrateMode.CBR: 'CBR',\n",
    "            mutagen.mp3.BitrateMode.VBR: 'VBR',\n",
    "            mutagen.mp3.BitrateMode.ABR: 'ABR',\n",
    "            mutagen.mp3.BitrateMode.UNKNOWN: 'UNKNOWN',\n",
    "        }\n",
    "\n",
    "        metadata['bit_rate'] = f.info.bitrate\n",
    "        metadata['mode'] = mode[f.info.bitrate_mode]\n",
    "        metadata['channels'] = f.info.channels\n",
    "        metadata['sample_rate'] = f.info.sample_rate\n",
    "        metadata['samples'] = x.shape[-1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print('{}: {}'.format(tid, repr(e)))\n",
    "        metadata['bit_rate'] = 0\n",
    "        metadata['mode'] = 'ERROR'\n",
    "        metadata['channels'] = 0\n",
    "        metadata['sample_rate'] = 0\n",
    "        metadata['samples'] = 0\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_mp3_metadata(args):\n",
    "    \"\"\"\n",
    "    Fill metadata about the audio, e.g. the bit and sample rates.\n",
    "    It extracts metadata from the mp3 and creates an mp3_metadata.csv table.\n",
    "    \"\"\"\n",
    "\n",
    "    # More than usable CPUs to be CPU bound, not I/O bound. Beware memory.\n",
    "    nb_workers = int(1.5 * len(os.sched_getaffinity(0)))\n",
    "    print('Working with {} processes.'.format(nb_workers))\n",
    "\n",
    "    path = os.path.join(args.path, 'fma_full')\n",
    "    tids = utils.get_tids_from_directory(path)\n",
    "\n",
    "    metadata = pd.DataFrame(index=tids)\n",
    "    metadata.index.name = 'track_id'\n",
    "    # Prevent the columns of being of type float because of NaNs.\n",
    "    metadata['channels'] = 0\n",
    "    metadata['mode'] = 'UNKNOWN'\n",
    "    metadata['bit_rate'] = 0\n",
    "    metadata['sample_rate'] = 0\n",
    "    metadata['samples'] = 0\n",
    "\n",
    "    pool = multiprocessing.Pool(nb_workers)\n",
    "    extract = partial(_extract_metadata, path=path)\n",
    "    it = pool.imap_unordered(extract, tids)\n",
    "\n",
    "    for row in tqdm(it, total=len(tids)):\n",
    "        metadata.loc[row.name] = row\n",
    "\n",
    "    not_found = pickle.load(open('not_found.pickle', 'rb'))\n",
    "    tids = list(metadata[metadata['mode'] == 'ERROR'].index)\n",
    "    not_found['mp3_metadata'] = tids\n",
    "    pickle.dump(not_found, open('not_found.pickle', 'wb'))\n",
    "\n",
    "    metadata.drop(tids, inplace=True)\n",
    "    metadata.sort_index(axis=0, inplace=True)\n",
    "    metadata.sort_index(axis=1, inplace=True)\n",
    "    metadata.to_csv('mp3_metadata.csv')\n",
    "\n",
    "\n",
    "def trim_audio(args):\n",
    "\n",
    "    path = os.path.abspath(args.path)\n",
    "    fma_full = os.path.join(path, 'fma_full')\n",
    "    fma_large = os.path.join(path, 'fma_large')\n",
    "    tracks = pd.read_csv('mp3_metadata.csv', index_col=0)\n",
    "    _create_subdirs(fma_large, tracks)\n",
    "\n",
    "    not_found = pickle.load(open('not_found.pickle', 'rb'))\n",
    "    not_found['clips'] = []\n",
    "\n",
    "    for tid, track in tqdm(tracks.iterrows(), total=len(tracks)):\n",
    "        duration = track['samples'] / track['sample_rate']\n",
    "        src = utils.get_audio_path(fma_full, tid)\n",
    "        dst = utils.get_audio_path(fma_large, tid)\n",
    "        if os.path.exists(dst):\n",
    "            continue\n",
    "        elif duration <= 30:\n",
    "            shutil.copyfile(src, dst)\n",
    "        else:\n",
    "            start = int(duration // 2 - 15)\n",
    "            command = ['ffmpeg', '-i', src,\n",
    "                       '-ss', str(start), '-t', '30',\n",
    "                       '-acodec', 'copy', dst]\n",
    "            try:\n",
    "                sp.run(command, check=True, stderr=sp.DEVNULL)\n",
    "            except sp.CalledProcessError:\n",
    "                not_found['clips'].append(tid)\n",
    "\n",
    "    for tid in not_found['clips']:\n",
    "        try:\n",
    "            os.remove(utils.get_audio_path(fma_large, tid))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    pickle.dump(not_found, open('not_found.pickle', 'wb'))\n",
    "\n",
    "\n",
    "def normalize_permissions_times(args):\n",
    "    path = os.path.abspath(args.path)\n",
    "    for dirpath, dirnames, filenames in tqdm(os.walk(path)):\n",
    "        for name in filenames:\n",
    "            dst = os.path.join(dirpath, name)\n",
    "            os.chmod(dst, 0o444)\n",
    "            os.utime(dst, (TIME, TIME))\n",
    "        for name in dirnames:\n",
    "            dst = os.path.join(dirpath, name)\n",
    "            os.chmod(dst, 0o555)\n",
    "            os.utime(dst, (TIME, TIME))\n",
    "\n",
    "\n",
    "def create_zips(args):\n",
    "\n",
    "    def get_filepaths(subset):\n",
    "        filepaths = []\n",
    "        tids = tracks.index[tracks['set', 'subset'] <= subset]\n",
    "        for tid in tids:\n",
    "            filepaths.append(utils.get_audio_path('', tid))\n",
    "        return filepaths\n",
    "\n",
    "    def get_checksums(base_dir, filepaths):\n",
    "        \"\"\"Checksums are assumed to be stored in order for efficiency.\"\"\"\n",
    "        checksums = []\n",
    "        with open(os.path.join(args.path, base_dir, 'checksums')) as f:\n",
    "            for filepath in filepaths:\n",
    "                exist = False\n",
    "                for line in f:\n",
    "                    if filepath == line[42:-1]:\n",
    "                        exist = True\n",
    "                        break\n",
    "                if not exist:\n",
    "                    raise ValueError('checksum not found: {}'.format(filepath))\n",
    "                checksums.append(line)\n",
    "        return checksums\n",
    "\n",
    "    def create_zip(zip_filename, base_dir, filepaths):\n",
    "\n",
    "        # Audio: all compressions are the same.\n",
    "        # CSV: stored > deflated > BZIP2 > LZMA.\n",
    "        # LZMA is close to BZIP2 and too recent to be widely available (unzip).\n",
    "        compression = zipfile.ZIP_BZIP2\n",
    "\n",
    "        zip_filepath = os.path.join(args.path, zip_filename)\n",
    "        with zipfile.ZipFile(zip_filepath, 'x', compression) as zf:\n",
    "\n",
    "            def info(name):\n",
    "                name = os.path.join(zip_filename[:-4], name)\n",
    "                info = zipfile.ZipInfo(name, (2017, 4, 1, 0, 0, 0))\n",
    "                info.external_attr = 0o444 << 16 | 0o2 << 30\n",
    "                return info\n",
    "\n",
    "            zf.writestr(info('README.txt'), README, compression)\n",
    "\n",
    "            checksums = get_checksums(base_dir, filepaths)\n",
    "            zf.writestr(info('checksums'), ''.join(checksums), compression)\n",
    "\n",
    "            for filepath in tqdm(filepaths):\n",
    "                src = os.path.join(args.path, base_dir, filepath)\n",
    "                dst = os.path.join(zip_filename[:-4], filepath)\n",
    "                zf.write(src, dst)\n",
    "\n",
    "        os.chmod(zip_filepath, 0o444)\n",
    "        os.utime(zip_filepath, (TIME, TIME))\n",
    "\n",
    "    METADATA = [\n",
    "        'not_found.pickle',\n",
    "        'raw_genres.csv', 'raw_albums.csv',\n",
    "        'raw_artists.csv', 'raw_tracks.csv',\n",
    "        'tracks.csv', 'genres.csv',\n",
    "        'raw_echonest.csv', 'echonest.csv', 'features.csv',\n",
    "    ]\n",
    "    create_zip('fma_metadata.zip', 'fma_metadata', METADATA)\n",
    "\n",
    "    tracks = utils.load('tracks.csv')\n",
    "    create_zip('fma_small.zip', 'fma_large', get_filepaths('small'))\n",
    "    create_zip('fma_medium.zip', 'fma_large', get_filepaths('medium'))\n",
    "    create_zip('fma_large.zip', 'fma_large', get_filepaths('large'))\n",
    "    create_zip('fma_full.zip', 'fma_full', get_filepaths('large'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    desc = 'Collect and process data to create the Free Music Archive (FMA) dataset.'\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "    subparsers = parser.add_subparsers(title='subcommands')\n",
    "\n",
    "    path = 'Path to the folder where the audio of the FMA subsets is stored.'\n",
    "\n",
    "    desc = ('Query the API of the FMA and store the collected metadata in '\n",
    "            'raw_tracks.csv, raw_albums.csv, raw_artists.csv, and '\n",
    "            'raw_genres.csv. The files are created in the current directory.')\n",
    "    subparser = subparsers.add_parser('metadata', description=desc)\n",
    "    subparser.add_argument('--min', dest='tid_min', type=int, default=0,\n",
    "                           help='smallest track ID to consider')\n",
    "    subparser.add_argument('--max', dest='tid_max', type=int, default=None,\n",
    "                           help='largest track ID to consider')\n",
    "    subparser.set_defaults(func=download_metadata)\n",
    "\n",
    "    desc = 'Download the mp3 audio of each track.'\n",
    "    subparser = subparsers.add_parser('data', description=desc)\n",
    "    subparser.add_argument('path', type=str, help=path)\n",
    "    subparser.set_defaults(func=download_data)\n",
    "\n",
    "    desc = 'Extract technical metadata, such as duration, from the audio.'\n",
    "    subparser = subparsers.add_parser('mp3_metadata', description=desc)\n",
    "    subparser.add_argument('path', type=str, help=path)\n",
    "    subparser.set_defaults(func=extract_mp3_metadata)\n",
    "\n",
    "    desc = 'Extract 30s clips from the downloaded full-length audio.'\n",
    "    subparser = subparsers.add_parser('clips', description=desc)\n",
    "    subparser.add_argument('path', type=str, help=path)\n",
    "    subparser.set_defaults(func=trim_audio)\n",
    "\n",
    "    desc = 'Normalize the file permissions and times.'\n",
    "    subparser = subparsers.add_parser('normalize', description=desc)\n",
    "    subparser.add_argument('path', type=str, help=path)\n",
    "    subparser.set_defaults(func=normalize_permissions_times)\n",
    "\n",
    "    desc = 'Create the datasets as ZIP archives.'\n",
    "    subparser = subparsers.add_parser('zips', description=desc)\n",
    "    subparser.add_argument('path', type=str, help=path)\n",
    "    subparser.set_defaults(func=create_zips)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.func(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
